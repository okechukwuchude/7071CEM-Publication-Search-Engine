{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mport libraries\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk. tokenize import RegexpTokenizer\n",
    "import schedule\n",
    "import urllib.robotparser\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "import webbrowser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Stopwords\n",
    "import requests\n",
    "\n",
    "# Download the file\n",
    "url = \"https://gist.githubusercontent.com/ZohebAbai/513218c3468130eacff6481f424e4e64/raw/b70776f341a148293ff277afa0d0302c8c38f7e2/gist_stopwords.txt\"\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Open the file and write the content\n",
    "    with open(\"gist_stopwords.txt\", \"w\") as gist_file:\n",
    "        gist_file.write(response.text)\n",
    "\n",
    "        # Read the content, split using commas, and remove double quotes and leading/trailing whitespaces\n",
    "        stopwords = [i.replace('\"', \"\").strip() for i in response.text.split(\",\")]\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower() # Convert the text to lowercase\n",
    "    processed_text = ''.join([char for char in text if char.isalpha() or char.isspace()]) # Remove punctuation, numbers, and symbols\n",
    "    tokens = tokenize_text(processed_text) # Tokenize the text\n",
    "    filtered_tokens = remove_stop_words(tokens) # Remove stop words from the tokens\n",
    "    stemmed_tokens = stem_words(filtered_tokens) # Stem the tokens\n",
    "    return stemmed_tokens\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "    return tokenizer.tokenize(text)\n",
    "\n",
    "def remove_stop_words(tokens):\n",
    "    stop_words = set(stopwords)\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "def stem_words(tokens):\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inverted Index\n",
    "def create_inverted_index(documents):\n",
    "\n",
    "    inverted_index = {} # Inverted index dictionary\n",
    "\n",
    "    document_id = 0 # Unique document ID for each record\n",
    "\n",
    "    for document in documents:\n",
    "        for record in document['records']:\n",
    "            document_id += 1\n",
    "\n",
    "            preprocessed_title = preprocess_text(record['title']) # Preprocess the crawled data\n",
    "            preprocessed_journal = preprocess_text(record['journal'])\n",
    "\n",
    "            # Build inverted index\n",
    "            for token in preprocessed_title + preprocessed_journal:\n",
    "                if token in inverted_index:\n",
    "                    inverted_index[token].append(document_id)\n",
    "                else:\n",
    "                    inverted_index[token] = [document_id]\n",
    "\n",
    "    return inverted_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Term Frequency - Inverse Document Frequency\n",
    "def calculate_tfidf(documents, inverted_index):\n",
    "\n",
    "    # TF-IDF (Term Frequency - Inverse Document Frequency) dictionary\n",
    "    tfidf_scores = {}\n",
    "\n",
    "    # Unique document ID for each record\n",
    "    document_id = 0\n",
    "\n",
    "    for document in documents:\n",
    "        for record in document['records']:\n",
    "            document_id += 1\n",
    "\n",
    "            # Preprocess the data\n",
    "            preprocessed_title = preprocess_text(record['title'])\n",
    "            preprocessed_journal = preprocess_text(record['journal'])\n",
    "\n",
    "            # Get the required tokens\n",
    "            document_tokens = preprocessed_title + preprocessed_journal\n",
    "\n",
    "            # Find out max word count in a record\n",
    "            max_frequency = max(document_tokens.count(w) for w in document_tokens)\n",
    "\n",
    "            # For each record, we store the tfidf for each term in another dict\n",
    "            tfidf_scores[document_id] = {}\n",
    "\n",
    "            for token in document_tokens:\n",
    "                tf = document_tokens.count(token) / max_frequency\n",
    "                idf = math.log(len(documents) / len(inverted_index[token]))\n",
    "                tfidf_scores[document_id][token] = tf * idf\n",
    "\n",
    "    return tfidf_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_documents(query, documents, inverted_index, tfidf_scores):\n",
    "\n",
    "    # Preprocess user query\n",
    "    query_tokens = preprocess_text(query)\n",
    "\n",
    "    # Compute the score based on tfidf\n",
    "    scores = {}\n",
    "    for token in query_tokens:\n",
    "        if token in inverted_index:\n",
    "            for document_id in inverted_index[token]:\n",
    "                if document_id in scores:\n",
    "                    scores[document_id] += tfidf_scores[document_id][token]\n",
    "                else:\n",
    "                    scores[document_id] = tfidf_scores[document_id][token]\n",
    "\n",
    "    # Return the sorted (descending) list of scores\n",
    "    return sorted(scores, key=scores.get, reverse=True)\n",
    "\n",
    "def can_fetch_url(url, user_agent='*'):\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(urllib.parse.urljoin(url, '/robots.txt'))\n",
    "    rp.read()\n",
    "    return rp.can_fetch(user_agent, url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_publications(url):\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Respect robots.txt and avoid overloading the server\n",
    "    time.sleep(5)\n",
    "\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\") # Parse data\n",
    "\n",
    "    # Extract all publication-related data\n",
    "    publication_containers = soup.find_all(\"div\", class_=\"result-container\")\n",
    "\n",
    "    publications = []\n",
    "    for container in publication_containers:\n",
    "\n",
    "        # Get title and publication link\n",
    "        publication_title = container.find(\"h3\", class_=\"title\").text\n",
    "        publication_link = container.find(\"a\", class_=\"link\")[\"href\"]\n",
    "\n",
    "        # Get authors and their profiles\n",
    "        author_elements = container.find_all(\"a\", class_=\"link person\")\n",
    "        authors = [author.text.strip() for author in author_elements]\n",
    "\n",
    "        author_profile_elements = container.find_all(\"a\", class_=\"link person\", rel=\"Person\")\n",
    "        author_profiles = [author[\"href\"] for author in author_profile_elements]\n",
    "\n",
    "        # Extract date, journal, volume, number of pages, and article ID\n",
    "        publication_date = container.find(\"span\", class_=\"date\").text\n",
    "        journal_element = container.find(\"span\", class_=\"journal\")\n",
    "        publication_journal = journal_element.text.strip() if journal_element else \"\"\n",
    "\n",
    "        volume_element = container.find(\"span\", class_=\"volume\")\n",
    "        publication_volume = volume_element.text.strip() if volume_element else \"\"\n",
    "\n",
    "        number_of_pages_element = container.find(\"span\", class_=\"numberofpages\")\n",
    "        number_of_pages = number_of_pages_element.text.strip() if number_of_pages_element else \"\"\n",
    "\n",
    "        article_id = container.find(\"p\", class_=\"type\").text.split()[-1]\n",
    "\n",
    "        # Construct a dictionary with all extracted information\n",
    "        publication_data = {\n",
    "            \"title\": publication_title,\n",
    "            \"publication_link\": publication_link,\n",
    "            \"authors\": authors,\n",
    "            \"author_profiles\": author_profiles,\n",
    "            \"date\": publication_date,\n",
    "            \"journal\": publication_journal,\n",
    "            \"volume\": publication_volume,\n",
    "            \"number_of_pages\": number_of_pages,\n",
    "            \"article_id\": article_id,\n",
    "        }\n",
    "\n",
    "        publications.append(publication_data)\n",
    "\n",
    "    return publications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_by_id(document_id, documents):\n",
    "    current_document_id = 0\n",
    "    for document in documents:\n",
    "        for record in document['records']:\n",
    "            current_document_id += 1\n",
    "            if current_document_id == document_id:\n",
    "                return record\n",
    "    return None\n",
    "\n",
    "def search_engine_gui(documents, inverted_index, tfidf_scores):\n",
    "    urls = {}  # Dictionary to store URLs associated with tags\n",
    "\n",
    "    # Set up the GUI\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Search Engine\")\n",
    "\n",
    "    def open_link(event):\n",
    "        index = result_text.index(\"@%d,%d\" % (event.x, event.y))\n",
    "        tag_name = result_text.tag_names(index)[0]\n",
    "        url = urls.get(tag_name)\n",
    "        if url:\n",
    "            webbrowser.open(url)\n",
    "\n",
    "    def search():\n",
    "        query = entry.get()\n",
    "        ranked_documents = rank_documents(query, documents, inverted_index, tfidf_scores)[:10]\n",
    "\n",
    "        result_text.delete(1.0, tk.END)  # Clear the result text box\n",
    "        for document_id in ranked_documents:\n",
    "            document_info = get_document_by_id(document_id, documents)\n",
    "            if document_info:\n",
    "                result_text.insert(tk.END, f\"Document ID: {document_id}\\nTitle: {document_info['title']}\\n\\n\")\n",
    "                link = document_info['publication_link']\n",
    "                result_text.insert(tk.END, f\"Publication Link: {link}\\n\\n\", f\"link{document_id}\")  # Tag the link\n",
    "                urls[f\"link{document_id}\"] = link  # Add the URL to the urls dictionary\n",
    "                result_text.tag_bind(f\"link{document_id}\", \"<Button-1>\", open_link)  # Bind the click event to the tag\n",
    "                result_text.tag_config(f\"link{document_id}\", foreground=\"blue\", underline=1)  # Style the link\n",
    "\n",
    "        # Display authors and their profile links\n",
    "            for i, (author, author_profile) in enumerate(zip(document_info['authors'], document_info['author_profiles'])):\n",
    "                author_link_tag = f\"author_link{document_id}_{i}\"\n",
    "                result_text.insert(tk.END, f\"Author: {author}, Profile Link: \", \"author_label\")\n",
    "                result_text.insert(tk.END, f\"{author_profile}\\n\", author_link_tag)\n",
    "                urls[author_link_tag] = author_profile\n",
    "                result_text.tag_bind(author_link_tag, \"<Button-1>\", open_link)\n",
    "                result_text.tag_config(author_link_tag, foreground=\"green\", underline=1)\n",
    "\n",
    "            result_text.insert(tk.END, \"\\n\")\n",
    "\n",
    "    # Create the label, entry, button, and text widgets\n",
    "    label = tk.Label(window, text=\"Enter your query:\")\n",
    "    label.pack()\n",
    "\n",
    "    entry = tk.Entry(window, width=50)\n",
    "    entry.pack()\n",
    "\n",
    "    search_button = tk.Button(window, text=\"Search\", command=search)\n",
    "    search_button.pack()\n",
    "\n",
    "    result_text = scrolledtext.ScrolledText(window, wrap=tk.WORD, width=100, height=50)\n",
    "\n",
    "    root.mainloop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_publications_and_save(json_filename):\n",
    "    publications = []\n",
    "    for page_number in range(10):\n",
    "        url = f\"https://pureportal.coventry.ac.uk/en/organisations/ics-research-centre-for-fluid-and-complex-systems-fcs/publications/?page={page_number}\"\n",
    "        publications_from_page = crawl_publications(url)\n",
    "        publications.extend(publications_from_page)\n",
    "\n",
    "    with open(json_filename, \"w\") as json_file:\n",
    "        json.dump(publications, json_file)\n",
    "\n",
    "    return publications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # This file will store all our crawled publication data\n",
    "    # This is to avoid multiple crawlings every time the code is run\n",
    "    json_filename = \"publications.json\"\n",
    "\n",
    "    # Check if the JSON file exists or not\n",
    "    if os.path.exists(json_filename) and os.path.isfile(json_filename):\n",
    "\n",
    "        # If it exists, then just open the file and load all data into the publications list\n",
    "        with open(json_filename, \"r\") as json_file:\n",
    "            publications = json.load(json_file)\n",
    "    else:\n",
    "        # Otherwise, perform web crawling and populate publications\n",
    "        # Crawl 10 web pages to get all publication information\n",
    "        publications = crawl_publications_and_save(json_filename)\n",
    "\n",
    "    # The publications list contains records in the following format:\n",
    "    # publications[i][j] is a dictionary containing data of the j-th publication present on the i-th page\n",
    "    # For example, publications[0][0] returns the first record present on the first page\n",
    "\n",
    "    # Call the relevant functions to create an index and calculate TF-IDF\n",
    "    index = create_inverted_index(publications)\n",
    "    tfidf_scores = calculate_tfidf(publications, index)\n",
    "\n",
    "    # Prompt the user for a query\n",
    "    query = input(\"Enter your query: \")\n",
    "\n",
    "    # Display only the top 10 most relevant search results\n",
    "    ranked_documents = rank_documents(query, publications, index, tfidf_scores)[:10]\n",
    "\n",
    "        # Display the relevant information of the retrieved documents\n",
    "    for document_id in ranked_documents:\n",
    "        # Find the corresponding document record\n",
    "        found_record = None\n",
    "        for publication in publications:\n",
    "            for record in publication['records']:\n",
    "                if record['article_id'] == document_id:\n",
    "                    found_record = record\n",
    "                    break\n",
    "\n",
    "        if found_record:\n",
    "            print(\"\\nDocument ID:\", document_id)\n",
    "            print(\"Title:\", found_record['title'])\n",
    "            print(\"Publication link:\", found_record['publication_link'])\n",
    "            print(\"Authors:\", found_record['authors'])\n",
    "            print(\"Author's Profile:\", found_record['author_profiles'])\n",
    "            print(\"Date:\", found_record['date'])\n",
    "            print(\"Journal:\", found_record['journal'])\n",
    "            print(\"Volume:\", found_record['volume'])\n",
    "            print(\"Article ID:\", found_record['article_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'records'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\okechukwu chude\\Documents\\Information Retrival\\7071CEM-Publication-Search-Engine\\information retreival.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     publications \u001b[39m=\u001b[39m crawl_publications_and_save(json_filename)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Create an inverted index and calculate TF-IDF\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m inverted_index \u001b[39m=\u001b[39m create_inverted_index(publications)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m tfidf_scores \u001b[39m=\u001b[39m calculate_tfidf(publications, inverted_index)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Run the GUI-based search engine\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\okechukwu chude\\Documents\\Information Retrival\\7071CEM-Publication-Search-Engine\\information retreival.ipynb Cell 11\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m document_id \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39m# Unique document ID for each record\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfor\u001b[39;00m document \u001b[39min\u001b[39;00m documents:\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival.ipynb#X15sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mfor\u001b[39;00m record \u001b[39min\u001b[39;00m document[\u001b[39m'\u001b[39;49m\u001b[39mrecords\u001b[39;49m\u001b[39m'\u001b[39;49m]:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         document_id \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         preprocessed_title \u001b[39m=\u001b[39m preprocess_text(record[\u001b[39m'\u001b[39m\u001b[39mtitle\u001b[39m\u001b[39m'\u001b[39m]) \u001b[39m# Preprocess the crawled data\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'records'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Assuming 'publications.json' is defined above\n",
    "    json_filename = \"publications.json\"\n",
    "\n",
    "    # Check if the JSON file exists\n",
    "    if os.path.exists(json_filename) and os.path.isfile(json_filename):\n",
    "        # Load data from the JSON file\n",
    "        with open(json_filename, \"r\") as json_file:\n",
    "            publications = json.load(json_file)\n",
    "    else:\n",
    "        # Crawl and save the data\n",
    "        publications = crawl_publications_and_save(json_filename)\n",
    "\n",
    "    # Create an inverted index and calculate TF-IDF\n",
    "    inverted_index = create_inverted_index(publications)\n",
    "    tfidf_scores = calculate_tfidf(publications, inverted_index)\n",
    "\n",
    "    # Run the GUI-based search engine\n",
    "    search_engine_gui(publications, inverted_index, tfidf_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
