{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "import webbrowser\n",
    "import requests\n",
    "import urllib.parse\n",
    "import urllib.robotparser\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Define Stopwords ####################\n",
    "import requests\n",
    "\n",
    "# Download the file\n",
    "url = \"https://gist.githubusercontent.com/ZohebAbai/513218c3468130eacff6481f424e4e64/raw/b70776f341a148293ff277afa0d0302c8c38f7e2/gist_stopwords.txt\"\n",
    "crawl_response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful\n",
    "if crawl_response.status_code == 200:\n",
    "    # Open the file and write the content\n",
    "    with open(\"gist_stopwords.txt\", \"w\") as gist_file:\n",
    "        gist_file.write(crawl_response.text)\n",
    "\n",
    "        # Read the content, split using commas, and remove double quotes and leading/trailing whitespaces\n",
    "        stopwords = [i.replace('\"', \"\").strip() for i in crawl_response.text.split(\",\")]\n",
    "\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {crawl_response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############  text Preprocessing ###############\n",
    "\n",
    "def preprocess_text (text):\n",
    "    text = text.lower()    # Lowercase the text\n",
    "    text = ''.join([c for c in text if c.isalpha() or c.isspace()])   # Remove any punctuations, numbers or symbols \n",
    "\n",
    "    # tokenize, remove stopwords, and stemming\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    stop_words = set(stopwords)\n",
    "    words = [w for w in tokens if w not in stop_words]\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    words = [stemmer.stem(w) for w in words]\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Inverted Index ##################\n",
    "def create_inverted_index(documents):\n",
    "\n",
    "    inverted_index = {} # Inverted index dictionary\n",
    "\n",
    "    document_id = 0 # Unique document ID for each record\n",
    "\n",
    "    for document in documents:\n",
    "        for record in document:\n",
    "            document_id += 1\n",
    "\n",
    "            preprocessed_title = preprocess_text(record['title'])\n",
    "            preprocessed_journal = preprocess_text(record['journal'])\n",
    "\n",
    "            # Build inverted index\n",
    "            for token in preprocessed_title + preprocessed_journal:\n",
    "                if token in inverted_index:\n",
    "                    inverted_index[token].append(document_id)\n",
    "                else:\n",
    "                    inverted_index[token] = [document_id]\n",
    "\n",
    "    return inverted_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Term Frequency Inveders document Frequency#########\n",
    "def calculate_tfidf(information, index):\n",
    "\n",
    "    # TF-IDF (Term Frequency - Inverse Document Frequency) dict\n",
    "    tfidf_scores = {}\n",
    "\n",
    "    # Unique doc ID for each record\n",
    "    document_id = 0\n",
    "\n",
    "    for document in documents:\n",
    "        for record in document:\n",
    "            document_id += 1\n",
    "\n",
    "            # PReprocess the data\n",
    "            preprocessed_title = preprocess_text(record['title'])\n",
    "            preprocessed_journal = preprocess_text(record['journal'])\n",
    "\n",
    "            #Get the required tokens \n",
    "            document_tokens = preprocessed_title + preprocessed_journal\n",
    "\n",
    "            # Find out max word count in a record\n",
    "            max_frequency = max(document_tokens.count(w) for w in document_tokens)\n",
    "\n",
    "            # For each record, we store the tfidf for each term in another dict\n",
    "            tfidf_scores[document_id] = {}\n",
    "\n",
    "            for token in document_tokens:\n",
    "                tf = document_tokens.count(token) / max_frequency\n",
    "                idf = math.log(len(documents) / len(inverted_index[token]))\n",
    "                tfidf_scores[document_id][token] = tf * idf\n",
    "\n",
    "    return tfidf_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## TDIF SCORE RANK ################\n",
    "def rank_documents(query, information, index, tfidf):\n",
    "\n",
    "    # Preprocess user query\n",
    "    query_tokens = preprocess(query)\n",
    "\n",
    "    # Compute the score based on tfidf\n",
    "    scores = {}\n",
    "    for token in query_tokens:\n",
    "        if token in index:\n",
    "            for doc_id in index[token]:\n",
    "                if doc_id in scores:\n",
    "                    scores[doc_id] += tfidf[doc_id][token]\n",
    "                else:\n",
    "                    scores[doc_id] = tfidf[doc_id][token]\n",
    "    \n",
    "    # Return the sorted (descending) list of scores\n",
    "    return sorted(scores, key=scores.get, reverse=True)\n",
    "\n",
    "def can_fetch(url, user_agent='*'):\n",
    "    rp = urllib.robotparser.RobotFileParser()\n",
    "    rp.set_url(urllib.parse.urljoin(url, '/robots.txt'))\n",
    "    rp.read()\n",
    "    return rp.can_fetch(user_agent, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### Crawling #################\n",
    "\n",
    "def mycrawler(url):\n",
    "\n",
    "    crawl_crawl_response = requests.get(url)\n",
    "\n",
    "    # Must be polite by preserving robots.txt rules and not hitting the servers too fast\n",
    "    time.sleep(5)\n",
    "\n",
    "    Bautsoup = Beautifulsoup(crawl_response.text, \"html.parser\") # Parse data\n",
    "\n",
    "    # Find all the publications related data\n",
    "    publication_containers = Bautsoup.find_all(\"div\", class_=\"result-container\")\n",
    "\n",
    "    publications = []\n",
    "    for container in publication_containers:\n",
    "\n",
    "        #publication title link\n",
    "        publication_title = container.find(\"h3\", class_=\"title\").text\n",
    "        publication_link = container.find(\"a\", class_=\"link\")[\"href\"]\n",
    "\n",
    "        # Get authors\n",
    "        author_elements = container.find_all(\"a\", class_=\"link person\")\n",
    "        authors = [author.text.strip() for author in author_elements]\n",
    "\n",
    "        author_profile_elements = container.find_all(\"a\", class_=\"link person\", rel=\"Person\")\n",
    "        author_profiles = [author[\"href\"] for author in author_profile_elements]\n",
    "\n",
    "         # Extract date, journal, volume, number of pages, and article ID\n",
    "        publication_date = container.find(\"span\", class_=\"date\").text\n",
    "        journal_element = container.find(\"span\", class_=\"journal\")\n",
    "        publication_journal = journal_element.text.strip() if journal_element else \"\"\n",
    "\n",
    "        volume_element = container.find(\"span\", class_=\"volume\")\n",
    "        publication_volume = volume_element.text.strip() if volume_element else \"\"\n",
    "\n",
    "        number_of_pages_element = container.find(\"span\", class_=\"numberofpages\")\n",
    "        number_of_pages = number_of_pages_element.text.strip() if number_of_pages_element else \"\"\n",
    "\n",
    "        article_id = container.find(\"p\", class_=\"type\").text.split()[-1]\n",
    "\n",
    "        # Construct a dictionary with all extracted information\n",
    "        publication_data = {\n",
    "            \"title\": publication_title,\n",
    "            \"publication_link\": publication_link,\n",
    "            \"authors\": authors,\n",
    "            \"author_profiles\": author_profiles,\n",
    "            \"date\": publication_date,\n",
    "            \"journal\": publication_journal,\n",
    "            \"volume\": publication_volume,\n",
    "            \"number_of_pages\": number_of_pages,\n",
    "            \"article_id\": article_id,\n",
    "        }\n",
    "\n",
    "        publications.append(publication_data)\n",
    "\n",
    "    return publications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "############## speedrun###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_by_id(document_id, documents):\n",
    "    current_document_id = 0\n",
    "    for document in documents:\n",
    "        for record in document:\n",
    "            current_document_id += 1\n",
    "            if current_document_id == document_id:\n",
    "                return record\n",
    "    return None\n",
    "\n",
    "def search_engine_gui(documents, inverted_index, tfidf_scores):\n",
    "    urls = {}  # Dictionary to store URLs associated with tags\n",
    "\n",
    "    # Set up the GUI\n",
    "    root = tk.Tk()\n",
    "    root.title(\"Search Engine\")\n",
    "\n",
    "    def open_link(event):\n",
    "        index = result_text.index(\"@%d,%d\" % (event.x, event.y))\n",
    "        tag_name = result_text.tag_names(index)[0]\n",
    "        url = urls.get(tag_name)\n",
    "        if url:\n",
    "            webbrowser.open(url)\n",
    "\n",
    "    def search():\n",
    "        query = entry.get()\n",
    "        ranked_documents = rank_documents(query, documents, inverted_index, tfidf_scores)[:10]\n",
    "\n",
    "        result_text.delete(1.0, tk.END)  # Clear the result text box\n",
    "        for document_id in ranked_documents:\n",
    "            document_info = get_document_by_id(document_id, documents)\n",
    "            if document_info:\n",
    "                result_text.insert(tk.END, f\"Document ID: {document_id}\\nTitle: {document_info['title']}\\n\\n\")\n",
    "                link = document_info['publication_link']\n",
    "                result_text.insert(tk.END, f\"Publication Link: {link}\\n\\n\", f\"link{document_id}\")  # Tag the link\n",
    "                urls[f\"link{document_id}\"] = link  # Add the URL to the urls dictionary\n",
    "                result_text.tag_bind(f\"link{document_id}\", \"<Button-1>\", open_link)  # Bind the click event to the tag\n",
    "                result_text.tag_config(f\"link{document_id}\", foreground=\"blue\", underline=1)  # Style the link\n",
    "\n",
    "        # Display authors and their profile links\n",
    "            for i, (author, author_profile) in enumerate(zip(document_info['authors'], document_info['author_profiles'])):\n",
    "                author_link_tag = f\"author_link{document_id}_{i}\"\n",
    "                result_text.insert(tk.END, f\"Author: {author}, Profile Link: \", \"author_label\")\n",
    "                result_text.insert(tk.END, f\"{author_profile}\\n\", author_link_tag)\n",
    "                urls[author_link_tag] = author_profile\n",
    "                result_text.tag_bind(author_link_tag, \"<Button-1>\", open_link)\n",
    "                result_text.tag_config(author_link_tag, foreground=\"green\", underline=1)\n",
    "\n",
    "            result_text.insert(tk.END, \"\\n\")\n",
    "\n",
    "    # Create the label, entry, button, and text widgets\n",
    "    label = tk.Label(root, text=\"Enter your query:\")\n",
    "    label.pack()\n",
    "\n",
    "    entry = tk.Entry(root, width=50)\n",
    "    entry.pack()\n",
    "\n",
    "    search_button = tk.Button(root, text=\"Search\", command=search)\n",
    "    search_button.pack()\n",
    "\n",
    "    result_text = scrolledtext.ScrolledText(root, wrap=tk.WORD, width=100, height=50)\n",
    "\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crawl_publications_and_save(json_filename):\n",
    "    publications = []\n",
    "    for page_number in range(10):\n",
    "        url = f\"https://pureportal.coventry.ac.uk/en/organisations/ics-research-centre-for-fluid-and-complex-systems-fcs/publications/?page={page_number}\"\n",
    "        publications_from_page = crawl_publications(url)\n",
    "        publications.extend(publications_from_page)\n",
    "\n",
    "    with open(json_filename, \"w\") as json_file:\n",
    "        json.dump(publications, json_file)\n",
    "\n",
    "    return publications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    # This file will store all our crawled publication data\n",
    "    # This is to avoid multiple crawlings every time the code is run\n",
    "    json_filename = \"publications.json\"\n",
    "\n",
    "    # Check if the JSON file exists or not\n",
    "    if os.path.exists(json_filename) and os.path.isfile(json_filename):\n",
    "\n",
    "        # If it exists, then just open the file and load all data into the publications list\n",
    "        with open(json_filename, \"r\") as json_file:\n",
    "            publications = json.load(json_file)\n",
    "    else:\n",
    "        # Otherwise, perform web crawling and populate publications\n",
    "        # Crawl 10 web pages to get all publication information\n",
    "        publications = crawl_publications_and_save(json_filename)\n",
    "\n",
    "    # The publications list contains records in the following format:\n",
    "    # publications[i][j] is a dictionary containing data of the j-th publication present on the i-th page\n",
    "    # For example, publications[0][0] returns the first record present on the first page\n",
    "\n",
    "    # Call the relevant functions to create an index and calculate TF-IDF\n",
    "    index = create_inverted_index(publications)\n",
    "    tfidf_scores = calculate_tfidf(publications, index)\n",
    "\n",
    "    # Prompt the user for a query\n",
    "    query = input(\"Enter your query: \")\n",
    "\n",
    "    # Display only the top 10 most relevant search results\n",
    "    ranked_documents = rank_documents(query, publications, index, tfidf_scores)[:10]\n",
    "\n",
    "        # Display the relevant information of the retrieved documents\n",
    "    for document_id in ranked_documents:\n",
    "        # Find the corresponding document record\n",
    "        found_record = None\n",
    "        for publication in publications:\n",
    "            for record in publication:\n",
    "                if record['article_id'] == document_id:\n",
    "                    found_record = record\n",
    "                    break\n",
    "\n",
    "        if found_record:\n",
    "            print(\"\\nDocument ID:\", document_id)\n",
    "            print(\"Title:\", found_record['title'])\n",
    "            print(\"Publication link:\", found_record['publication_link'])\n",
    "            print(\"Authors:\", found_record['authors'])\n",
    "            print(\"Author's Profile:\", found_record['author_profiles'])\n",
    "            print(\"Date:\", found_record['date'])\n",
    "            print(\"Journal:\", found_record['journal'])\n",
    "            print(\"Volume:\", found_record['volume'])\n",
    "            print(\"Article ID:\", found_record['article_id'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers, not 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\okechukwu chude\\Documents\\Information Retrival\\7071CEM-Publication-Search-Engine\\information retreival copy.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival%20copy.ipynb#X40sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     publications \u001b[39m=\u001b[39m crawl_publications_and_save(json_filename)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival%20copy.ipynb#X40sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Create an inverted index and calculate TF-IDF\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival%20copy.ipynb#X40sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m inverted_index \u001b[39m=\u001b[39m create_inverted_index(publications)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival%20copy.ipynb#X40sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m tfidf_scores \u001b[39m=\u001b[39m calculate_tfidf(publications, inverted_index)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival%20copy.ipynb#X40sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Run the GUI-based search engine\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\okechukwu chude\\Documents\\Information Retrival\\7071CEM-Publication-Search-Engine\\information retreival copy.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival%20copy.ipynb#X40sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfor\u001b[39;00m record \u001b[39min\u001b[39;00m document:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival%20copy.ipynb#X40sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     document_id \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival%20copy.ipynb#X40sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     preprocessed_title \u001b[39m=\u001b[39m preprocess_text(record[\u001b[39m'\u001b[39;49m\u001b[39mtitle\u001b[39;49m\u001b[39m'\u001b[39;49m]) \u001b[39m# Preprocess the crawled data\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival%20copy.ipynb#X40sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     preprocessed_journal \u001b[39m=\u001b[39m preprocess_text(record[\u001b[39m'\u001b[39m\u001b[39mjournal\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/okechukwu%20chude/Documents/Information%20Retrival/7071CEM-Publication-Search-Engine/information%20retreival%20copy.ipynb#X40sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# Build inverted index\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers, not 'str'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Assuming 'publications.json' is defined above\n",
    "    json_filename = \"publications.json\"\n",
    "\n",
    "    # Check if the JSON file exists\n",
    "    if os.path.exists(json_filename) and os.path.isfile(json_filename):\n",
    "        # Load data from the JSON file\n",
    "        with open(json_filename, \"r\") as json_file:\n",
    "            publications = json.load(json_file)\n",
    "    else:\n",
    "        # Crawl and save the data\n",
    "        publications = crawl_publications_and_save(json_filename)\n",
    "\n",
    "    # Create an inverted index and calculate TF-IDF\n",
    "    inverted_index = create_inverted_index(publications)\n",
    "    tfidf_scores = calculate_tfidf(publications, inverted_index)\n",
    "\n",
    "    # Run the GUI-based search engine\n",
    "    search_engine_gui(publications, inverted_index, tfidf_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
